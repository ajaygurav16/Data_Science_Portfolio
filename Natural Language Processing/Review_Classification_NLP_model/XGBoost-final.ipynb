{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler,OneHotEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import string, re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords   \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import string,re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=stopwords.words('english')\n",
    "ps=PorterStemmer()\n",
    "def cleatext(text):\n",
    "    \n",
    "    text=text.translate(text.maketrans(\"\",\"\",string.punctuation))\n",
    "    text=[ps.stem(w) for w in text.split(' ') if w not in sw]\n",
    "    text=' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def readdata(filename,fg):\n",
    "    df=pd.read_csv(filename)\n",
    "    review_title=df['Review Title']\n",
    "    review_text=df['Review Text']\n",
    "    topic=[]\n",
    "\n",
    "    review=df['Review Text']+df['Review Title']\n",
    "    if fg==1:\n",
    "        topic=df['topic']\n",
    "    review_text=review_text.apply(cleatext)\n",
    "    review_title=review_title.apply(cleatext)\n",
    "    review=review.apply(cleatext)\n",
    "    \n",
    "    return df,review,review_text,review_title,topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,review,review_text,review_title,topic=readdata('train.csv',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cv=CountVectorizer(max_features=1000)\n",
    "cv.fit(review_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=cv.transform(review_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdf=TfidfVectorizer(max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=2000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf.fit(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=tfdf.transform(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.hstack([x1.toarray(),x2.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5959, 3000)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addextra(review,X):\n",
    "    \n",
    "    snl=SentimentIntensityAnalyzer()\n",
    "    star=review.apply(lambda x:1 if \"*\" in x else 0)\n",
    "\n",
    "    smiley=review.apply(lambda x:1 if \":)\" in x else 0)\n",
    "    sad=review.apply(lambda x:1 if \":(\" in x else 0)\n",
    "    exp=review.apply(lambda x:1 if \"!\" in x else 0)\n",
    "    hash1=review.apply(lambda x:1 if \"#\" in x else 0)\n",
    "    comma1=review.apply(lambda x:1 if \",\" in x else 0)\n",
    "    dollar=review.apply(lambda x:1 if \"$\" in x else 0)\n",
    "\n",
    "    length1=review.apply(lambda x:len(x))\n",
    "    compound_polarity=review.apply(lambda x:snl.polarity_scores(x)['compound'])\n",
    "    \n",
    "    star=np.array(star).reshape(-1,1)\n",
    "    smiley=np.array(smiley).reshape(-1,1)\n",
    "    sad=np.array(sad).reshape(-1,1)\n",
    "    exp=np.array(exp).reshape(-1,1)\n",
    "    hash1=np.array(hash1).reshape(-1,1)\n",
    "    comma1=np.array(comma1).reshape(-1,1)\n",
    "    dollar=np.array(dollar).reshape(-1,1)\n",
    "    length1=np.array(length1).reshape(-1,1)\n",
    "    compound_polarity=np.array(compound_polarity).reshape(-1,1)\n",
    "    print(length1.shape)\n",
    "    print(X.shape)\n",
    "    X=np.hstack([X,star,smiley,sad,exp,hash1,comma1,dollar,length1,compound_polarity])\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "def addextra1(review,X):\n",
    "    \n",
    "    snl=SentimentIntensityAnalyzer()\n",
    "  \n",
    "\n",
    "    length1=review.apply(lambda x:len(x))\n",
    "    compound_polarity=review.apply(lambda x:snl.polarity_scores(x)['compound'])\n",
    "    \n",
    "    length1=np.array(length1).reshape(-1,1)\n",
    "    compound_polarity=np.array(compound_polarity).reshape(-1,1)\n",
    "\n",
    "    X=np.hstack([X,length1,compound_polarity])\n",
    "    print(X.shape)\n",
    "    return X\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 3002)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5959, 3002)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=addextra1(review,X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xg=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='multi:softprob',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=xg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5419463087248322"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg1=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 405 candidates, totalling 2025 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed: 78.6min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 333.6min\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed: 691.1min\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "#should evaluate by train_eval instead of the full dataset\n",
    "clf = GridSearchCV(xg1, params, n_jobs=4, \n",
    "                   cv=5, \n",
    "                   \n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train) \n",
    " \n",
    "#trust your CV! \n",
    "best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    xg=xgb.XGBClassifier()\n",
    "    xg.fit(X_train,y_train)\n",
    "    y_pred=xg.predict(X_test)\n",
    "    print(accuracy_score(y_pred,y_test))\n",
    "    return xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg=buildmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=pd.read_csv('test.csv')\n",
    "dt_review_text=dt['Review Text']\n",
    "dt_review_title=dt['Review Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2553, 3002)\n"
     ]
    }
   ],
   "source": [
    "dt,review_val,review_text_val,review_title_val,topic_val=readdata('test.csv',0)\n",
    "\n",
    "xt1=cv.transform(review_title_val)\n",
    "xt2=tfdf.transform(review_text_val)\n",
    "Xt=np.hstack([xt1.toarray(),xt2.toarray()])\n",
    "Xt=addextra1(review_val,Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt=sc.fit_transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=xg.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tp=pd.DataFrame({'Review Text':dt_review_text,'Review Title':dt_review_title,'topic':y_pred})\n",
    "tp.to_csv('Submission_XGBoost9.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
